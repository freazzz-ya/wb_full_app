import pandas as pd
import datetime
import time
import requests
import logging
import os
import random
import hashlib
from typing import List, Dict, Optional
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import io
import numpy as np
from collections import defaultdict

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

MOSCOW_TZ = datetime.timezone(datetime.timedelta(hours=3))

DATA_DIR = 'data_parser'
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

CONFIG = {
    'CITIES': ['–ú–æ—Å–∫–≤–∞'],
    'QUERIES_FILE': 'queries.txt',
    'MAX_PAGE': 10,
    'MAX_PAGE_sellers': 3,
    'BRANDS': ['YalowShop'],
    'SUPPLIERS': ['YalowShop'],
    'REQUEST_DELAY': 1,
    'DATA_FILE': os.path.join(DATA_DIR, 'positions_data.csv'),
    'CATEGORY_HISTORY_FILE': os.path.join(DATA_DIR, 'category_history.csv'),
    'AVG_POSITIONS_FILE': os.path.join(DATA_DIR, 'avg_positions_data.csv'),
    'GLOBAL_AVG_FILE': os.path.join(DATA_DIR, 'global_avg_positions.csv')
}

# –°–≤–µ–∂–∏–µ User-Agents –∏ –ø—Ä–æ–∫—Å–∏ –ø–æ–¥—Ö–æ–¥
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]

class WBParser:
    def __init__(self):
        self.session = requests.Session()
        self._update_headers()
        
    def _update_headers(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º User-Agent"""
        user_agent = random.choice(USER_AGENTS)
        self.session.headers.update({
            'User-Agent': user_agent,
            'Accept': '*/*',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.wildberries.ru/',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-site',
        })

    def load_queries(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(CONFIG['QUERIES_FILE']):
            raise FileNotFoundError(f"–§–∞–π–ª —Å –∑–∞–ø—Ä–æ—Å–∞–º–∏ '{CONFIG['QUERIES_FILE']}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        with open(CONFIG['QUERIES_FILE'], 'r', encoding='utf-8') as f:
            queries = [line.strip() for line in f if line.strip()]
        
        if not queries:
            raise ValueError(f"–§–∞–π–ª '{CONFIG['QUERIES_FILE']}' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        return queries

    def parse_products(self, query):
        """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ API —Å –æ–±—Ö–æ–¥–æ–º –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫"""
        results = []
        print(f"üîç –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥: '{query}'")
        
        for page in range(1, CONFIG['MAX_PAGE'] + 1):
            try:
                # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                time.sleep(CONFIG['REQUEST_DELAY'] + random.uniform(0.1, 0.3))
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                self._update_headers()
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω—ã–µ dest_id –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
                dest_ids = [-1257786, -1029256, -102269, -2162196, -1257786]
                dest_id = random.choice(dest_ids)
                
                # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                params = {
                    'ab_testing': 'false',
                    'appType': 1,
                    'curr': 'rub',
                    'dest': dest_id,
                    'query': query,
                    'resultset': 'catalog',
                    'sort': 'popular',
                    'spp': 30,
                    'uclusters': 1,
                    'page': page,
                    'lang': 'ru',
                    'locale': 'ru',
                    'timestamp': int(time.time() * 1000)
                }
                
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
                endpoints = [
                    'https://search.wb.ru/exactmatch/ru/common/v4/search',
                    'https://search.wb.ru/exactmatch/sng/common/v4/search',
                ]
                
                response = None
                for endpoint in endpoints:
                    try:
                        response = self.session.get(
                            endpoint,
                            params=params,
                            timeout=10
                        )
                        
                        if response.status_code == 200:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ
                            data = response.json()
                            if data and 'data' in data and 'products' in data['data']:
                                break
                    except:
                        continue
                
                if not response or response.status_code != 200:
                    print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page} –¥–ª—è '{query}'")
                    continue
                    
                data = response.json()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–≤–∞—Ä—ã
                products = data.get('data', {}).get('products', [])
                
                if not products:
                    break
                    
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–≤–∞—Ä—ã
                page_target_count = 0
                for idx, product in enumerate(products):
                    if self.is_target_product(product):
                        global_idx = (page - 1) * 100 + idx + 1
                        results.append(self.process_product(product, query, page, idx, global_idx))
                        page_target_count += 1
                
                print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤, {page_target_count} —Ü–µ–ª–µ–≤—ã—Ö")
                
                # –ï—Å–ª–∏ —Ç–æ–≤–∞—Ä–æ–≤ –º–µ–Ω—å—à–µ 100, –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                if len(products) < 100:
                    break
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {e}")
                continue
        
        print(f"‚úÖ –ó–∞–ø—Ä–æ—Å '{query}': {len(results)} —Ü–µ–ª–µ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
        return results

    def is_target_product(self, product):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ–≤–∞—Ä —Ü–µ–ª–µ–≤—ã–º"""
        if not isinstance(product, dict):
            return False
            
        brand = product.get('brand', '').strip().lower()
        supplier = product.get('supplier', '').strip()
        return brand in [b.lower() for b in CONFIG['BRANDS']] or supplier in CONFIG['SUPPLIERS']

    def process_product(self, product, query, page, idx, global_idx):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞"""
        log_data = product.get('log', {})
        
        return {
            '–ù–∞–∑–≤–∞–Ω–∏–µ': product.get('name', ''),
            '–ü–æ–∑–∏—Ü–∏—è': global_idx,
            '–ü—Ä–æ–º–æ –ø–æ–∑–∏—Ü–∏—è': log_data.get('promoPosition'),
            '–û—Ä–≥. –ø–æ–∑–∏—Ü–∏—è': log_data.get('position', idx + 1),
            '–ó–∞–ø—Ä–æ—Å': query,
            '–î–∞—Ç–∞': datetime.datetime.now(MOSCOW_TZ),
            '–ü—Ä–æ–º–æ': '–î–∞' if log_data.get('promoPosition') is not None else '–ù–µ—Ç',
            '–ì–æ—Ä–æ–¥': '–ú–æ—Å–∫–≤–∞',
            '–ê—Ä—Ç–∏–∫—É–ª': product.get('id', ''),
            '–ë—Ä–µ–Ω–¥': product.get('brand', ''),
            '–ü–æ—Å—Ç–∞–≤—â–∏–∫': product.get('supplier', ''),
            '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': product.get('subjectName', product.get('entity', '')),
            '–¶–µ–Ω–∞': product.get('salePriceU', '') // 100 if product.get('salePriceU') else ''
        }

class WBAnalytics:
    def __init__(self, data_file: str):
        self.data_file = data_file
        self.avg_positions_file = CONFIG['AVG_POSITIONS_FILE']
        self.global_avg_file = CONFIG['GLOBAL_AVG_FILE']
        self.df = self._load_data()
        self.avg_df = self._load_avg_data()
        self.global_avg_df = self._load_global_avg_data()

    def _load_global_avg_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—â–∏—Ö —Å—Ä–µ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏–π"""
        try:
            if os.path.exists(self.global_avg_file):
                df = pd.read_csv(self.global_avg_file)
                df['–î–∞—Ç–∞'] = pd.to_datetime(df['–î–∞—Ç–∞'])
                return df
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—â–∏—Ö —Å—Ä–µ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏–π: {e}")
        return pd.DataFrame(columns=['–î–∞—Ç–∞', '–°—Ä–µ–¥–Ω—è—è_–ø–æ–∑–∏—Ü–∏—è'])

    def update_category_history(self, category):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Å—Ä–µ–¥–Ω–µ–π –ø–æ–∑–∏—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            if self.df.empty:
                logger.warning("DataFrame –ø—É—Å—Ç")
                return False
                
            category_df = self.df[self.df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'] == category]
            if category_df.empty:
                logger.warning(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {category} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
                
            current_avg = round(category_df['–ü–æ–∑–∏—Ü–∏—è'].mean(), 1)
            current_time = datetime.datetime.now(MOSCOW_TZ)
            
            history_file = CONFIG['CATEGORY_HISTORY_FILE']
            
            new_record = {
                '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': category,
                '–î–∞—Ç–∞': current_time.strftime('%Y-%m-%d %H:%M:%S'),
                '–°—Ä–µ–¥–Ω—è—è_–ø–æ–∑–∏—Ü–∏—è': current_avg
            }
            
            if os.path.exists(history_file):
                try:
                    existing_data = pd.read_csv(history_file)
                    updated_data = pd.concat([existing_data, pd.DataFrame([new_record])], ignore_index=True)
                    logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞–ø–∏—Å—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º {len(existing_data)}")
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∏—Å—Ç–æ—Ä–∏–∏: {e}, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π")
                    updated_data = pd.DataFrame([new_record])
            else:
                updated_data = pd.DataFrame([new_record])
                logger.info("–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π —Ñ–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏")
            
            updated_data.to_csv(history_file, index=False)
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∞ –∑–∞–ø–∏—Å—å: {category} - {current_avg}. –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(updated_data)}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}", exc_info=True)
            return False

    def update_global_avg_positions(self, new_data: pd.DataFrame):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—â–∏—Ö —Å—Ä–µ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏–π"""
        try:
            if new_data.empty:
                return

            global_avg = round(new_data['–ü–æ–∑–∏—Ü–∏—è'].mean(), 1)
            current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            new_record = pd.DataFrame({
                '–î–∞—Ç–∞': [current_time],
                '–°—Ä–µ–¥–Ω—è—è_–ø–æ–∑–∏—Ü–∏—è': [global_avg]
            })

            if os.path.exists(self.global_avg_file):
                history = pd.read_csv(self.global_avg_file)
                updated = pd.concat([history, new_record])
            else:
                updated = new_record

            updated.to_csv(self.global_avg_file, index=False)
            logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω—ã –æ–±—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π. –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {global_avg}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—â–∏—Ö —Å—Ä–µ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏–π: {e}")

    def _load_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if os.path.exists(self.data_file):
                df = pd.read_csv(self.data_file)
                df['–î–∞—Ç–∞'] = pd.to_datetime(df['–î–∞—Ç–∞'])
                df['–ê—Ä—Ç–∏–∫—É–ª'] = df['–ê—Ä—Ç–∏–∫—É–ª'].astype(str)
                    
                logger.info(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(df)} –∑–∞–ø–∏—Å–µ–π")
                return df
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return pd.DataFrame(columns=['–ê—Ä—Ç–∏–∫—É–ª', '–ù–∞–∑–≤–∞–Ω–∏–µ', '–ö–∞—Ç–µ–≥–æ—Ä–∏—è', '–ü–æ–∑–∏—Ü–∏—è', '–î–∞—Ç–∞', '–ó–∞–ø—Ä–æ—Å', '–ü—Ä–æ–º–æ', '–¶–µ–Ω–∞', '–ë—Ä–µ–Ω–¥'])

    def _load_avg_data(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å—Ä–µ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏–π"""
        try:
            if os.path.exists(self.avg_positions_file):
                df = pd.read_csv(self.avg_positions_file)
                df['–î–∞—Ç–∞'] = pd.to_datetime(df['–î–∞—Ç–∞'])
                return df
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ä–µ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏–π: {e}")
        return pd.DataFrame(columns=['–ê—Ä—Ç–∏–∫—É–ª', '–°—Ä–µ–¥–Ω—è—è_–ø–æ–∑–∏—Ü–∏—è', '–î–∞—Ç–∞'])

    def update_avg_positions(self, new_data: pd.DataFrame):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏–π –ë–ï–ó –æ—á–∏—Å—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
        try:
            if new_data.empty:
                return

            current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            new_avg = new_data.groupby('–ê—Ä—Ç–∏–∫—É–ª').agg({
                '–ü–æ–∑–∏—Ü–∏—è': 'mean'
            }).reset_index()
            new_avg['–°—Ä–µ–¥–Ω—è—è_–ø–æ–∑–∏—Ü–∏—è'] = new_avg['–ü–æ–∑–∏—Ü–∏—è'].round(1)
            new_avg['–î–∞—Ç–∞'] = current_time
            new_avg = new_avg[['–ê—Ä—Ç–∏–∫—É–ª', '–°—Ä–µ–¥–Ω—è—è_–ø–æ–∑–∏—Ü–∏—è', '–î–∞—Ç–∞']]

            if os.path.exists(self.avg_positions_file):
                existing_data = pd.read_csv(self.avg_positions_file)
            else:
                existing_data = pd.DataFrame(columns=['–ê—Ä—Ç–∏–∫—É–ª', '–°—Ä–µ–¥–Ω—è—è_–ø–æ–∑–∏—Ü–∏—è', '–î–∞—Ç–∞'])

            updated_data = pd.concat([existing_data, new_avg], ignore_index=True)
            updated_data.to_csv(self.avg_positions_file, index=False)
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω—ã –Ω–æ–≤—ã–µ —Å—Ä–µ–¥–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏. –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(updated_data)}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–∏—Ö –ø–æ–∑–∏—Ü–∏–π: {e}")

    def get_available_articles(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—Ä—Ç–∏–∫—É–ª–æ–≤"""
        return sorted(self.df['–ê—Ä—Ç–∏–∫—É–ª'].astype(str).unique().tolist()) if not self.df.empty else []

    def get_product_data(self, article: str):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–æ–≤–∞—Ä—É"""
        try:
            article_str = str(article).strip()
            product_data = self.df[
                (self.df['–ê—Ä—Ç–∏–∫—É–ª'].astype(str) == article_str) | 
                (self.df['–ê—Ä—Ç–∏–∫—É–ª'].astype(int).astype(str) == article_str)
            ]
            
            if product_data.empty:
                return None, None
                
            stats = {
                'name': product_data['–ù–∞–∑–≤–∞–Ω–∏–µ'].iloc[0],
                'category': product_data['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].iloc[0],
                'first_check': product_data['–î–∞—Ç–∞'].min().strftime('%d.%m.%Y'),
                'last_check': product_data['–î–∞—Ç–∞'].max().strftime('%d.%m.%Y'),
                'queries_count': product_data['–ó–∞–ø—Ä–æ—Å'].nunique(),
                'avg_position': round(product_data['–ü–æ–∑–∏—Ü–∏—è'].mean(), 1),
                'best_position': product_data['–ü–æ–∑–∏—Ü–∏—è'].min(),
                'worst_position': product_data['–ü–æ–∑–∏—Ü–∏—è'].max(),
                'promo_percentage': round((product_data['–ü—Ä–æ–º–æ'] == '–î–∞').mean() * 100, 1)
            }
            
            return product_data, stats
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞: {e}")
            return None, None

    def get_available_categories(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        try:
            return self.df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è'].unique().tolist()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
            return []

    def get_available_queries(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            return self.df['–ó–∞–ø—Ä–æ—Å'].unique().tolist()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return []

class WBParserService:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞—Ä—Å–µ—Ä–æ–º"""
    
    def __init__(self):
        self.parser = WBParser()
        self.previous_data = None
        self.current_data = None
        self.last_check_time = None

    def check_positions(self):
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∑–∏—Ü–∏–π —á–µ—Ä–µ–∑ API"""
        print("=" * 60)
        print("üöÄ –ë–´–°–¢–†–´–ô –ü–ê–†–°–ò–ù–ì –ß–ï–†–ï–ó API")
        print("=" * 60)
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª –∑–∞–ø—Ä–æ—Å–æ–≤
            print("üìÅ –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª –∑–∞–ø—Ä–æ—Å–æ–≤...")
            queries = self.parser.load_queries()
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(queries)}")
            
            # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            data = []
            
            for i, query in enumerate(queries, 1):
                print(f"\n{'='*40}")
                print(f"üìã –ó–ê–ü–†–û–° {i}/{len(queries)}: '{query}'")
                print(f"{'='*40}")
                
                products = self.parser.parse_products(query)
                data.extend(products)
                print(f"üì¶ –ò—Ç–æ–≥–æ: {len(products)} —Ü–µ–ª–µ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
                
                # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if i < len(queries):
                    time.sleep(0.5)
            
            print(f"\n{'='*60}")
            print("üìä –°–í–û–î–ö–ê")
            print(f"{'='*60}")
            
            if not data:
                print("‚ùå –ù–ï –£–î–ê–õ–û–°–¨ –°–û–ë–†–ê–¢–¨ –î–ê–ù–ù–´–ï!")
                return False
                
            print(f"‚úÖ –£–°–ü–ï–®–ù–û: {len(data)} —Ü–µ–ª–µ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            self.current_data = pd.DataFrame(data)
            self.current_data.to_csv(CONFIG['DATA_FILE'], index=False)
            print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {CONFIG['DATA_FILE']}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∞–Ω–∞–ª–∏—Ç–∏–∫—É
            print("üìà –û–±–Ω–æ–≤–ª—è–µ–º –∞–Ω–∞–ª–∏—Ç–∏–∫—É...")
            analytics = WBAnalytics(CONFIG['DATA_FILE'])
            analytics.update_avg_positions(self.current_data)
            analytics.update_global_avg_positions(self.current_data)
            print("‚úÖ –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
            
            print(f"\nüéâ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –ó–ê {len(queries) * CONFIG['MAX_PAGE'] * CONFIG['REQUEST_DELAY']} –°–ï–ö–£–ù–î!")
            return True
            
        except Exception as e:
            print(f"\nüí• –û–®–ò–ë–ö–ê: {e}")
            import traceback
            traceback.print_exc()
            return False

    def load_queries(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞"""
        return self.parser.load_queries()

    def get_analytics(self):
        """–ü–æ–ª—É—á–∏—Ç—å –æ–±—ä–µ–∫—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        return WBAnalytics(CONFIG['DATA_FILE'])

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    service = WBParserService()
    
    try:
        while True:
            print("\n" + "="*50)
            print("üõçÔ∏è  Wildberries Parser (–ë–´–°–¢–†–´–ô)")
            print("="*50)
            print("1. üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ (–ë—ã—Å—Ç—Ä–æ)")
            print("2. üìä –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø –ø—Ä–æ–¥–∞–≤—Ü–æ–≤") 
            print("3. üìà –ü–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É")
            print("4. üö™ –í—ã—Ö–æ–¥")
            print("="*50)
            
            choice = input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1-4): ").strip()
            
            if choice == "1":
                service.check_positions()
            elif choice == "2":
                print("‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ —Ç–æ–ø –ø—Ä–æ–¥–∞–≤—Ü–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            elif choice == "3":
                analytics = service.get_analytics()
                print(f"üìä –î–æ—Å—Ç—É–ø–Ω–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤: {len(analytics.get_available_articles())}")
                print(f"üìÅ –î–æ—Å—Ç—É–ø–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(analytics.get_available_categories())}")
                print(f"üîç –î–æ—Å—Ç—É–ø–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(analytics.get_available_queries())}")
            elif choice == "4":
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            else:
                print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
            
            input("\n–ù–∞–∂–º–∏—Ç–µ Enter...")
    except KeyboardInterrupt:
        print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ...")

if __name__ == '__main__':
    main()